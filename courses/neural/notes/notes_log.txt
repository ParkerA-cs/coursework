MTSU
#neuron

----------------------------------------------------------1/21/2020---------------------------------------------------------------

UN PW for protected portions of the website

takes advantage or parallel processing in order to replicate the "thinking" processes of the human brain
parts work together in tandem rather than a more linear and functional program

	data in -> algorithm -> data out

this is the more normal type of programming

	data in -> untrained algorithm -> data out(often bad)
	with time VVVVVVVVV
	data in -> trained algorithm -> data out much more algorithm

Neural Nets are for problems that are difficult to code by hand

nodes ~~ neurons
Main system behaviour is often learned rather than program

born -> move limbs -> flip over -> crawl -> sit -> walk -> walk faster

learning is the optimization of a task based on a given set of tools and times
also occurs while task is being experienced real life or not
useful is what makes the task as optimal as possible

important in predicting credit card fraud

base concept an algorithm that codes itself based off of exceptions

-------------------------------------------------1/28/2020--------------------------------------------------------------------------

benefits of knowning CS history
	History prevents redundant repetition
	pioneers help to serve as guide stones
	lol cool

primitive Computer Scientists have always seen parallels between computers and people
	pros/cons to brains vs. computers

neurals nets just closer to human thought than other studies of artificial intelligence

trinity supercomputer los alamos nat lab vs house fly
super computer is faster
	operations
	transforspeeds
housefly is more efficient
	less neurons
	less tdp

what makes neurons special
	slow low bandwidth and imprecise
		no exception handling allows for sensory problems that exist

	cell death, but natural degradation

	always working in parallel with itself

	learning via synapse modding

	connection pathways topologically structured

	there is a spatial orientation to neuron placement

neuron computation
	the speed of a neuron is slow only 100 pulse per second

	think like recognizing someone 
	near instaneaneous therefore what is the algorithm that determines that
	less than 100 steps if one neuron
	multiple neurons work in tandem
	maybe millions

neurons can implement the moment it is made available

brains kind of equal computers

initial discoveries

pandemonium
adaline
theory of NA reinforcement systems
principles of neurodynamics

the dark ages of symbolic systems become preferred...

minsky and papert
plagairized and slandered neural nets

things get deep
just a whole lot of crazy developments lolol
GPU's mainly

neurons at their very base are dedicated feature detectors that are designed to detect for a singular feature
a single neuron works in conjuction to detect for a multitude of other neurons receiving and sending information to other neurons to acheive a task
information in(excitatory information going in) -> 
against other information that is already present to trigger or not trigger the neuron -> 
information out (excitatory information going on to the next neuron, either hi pulse or lo pulse) -> (repeat)

--------------------------------------------------------------------------------------------------------------------------------------------------------------
Rate Coding Activation Functions
Even though action potentials are discrete events they are often summarized together as a firing rate
Zero menas no APs and one means firing at max rate 
noise therefore we can refer to the following equation
y=1/(1+x^-1)

Memory
information or memory is stored in the connections
Summarized as the 'weight' of the connection(large/small connection)

we can summarize firing weights of neurons as the above equation 
and the change in neurons as 

we will have a number that will describe the strength of that connection

the net input for unit i can be "folded" into a single value, net of j, which weights the activation
of the sending neuron x of i by the strength of the synaptic connection between i and j, weight of i and j

the sum of all weighted activations into a unit plus a bias weight, w of O, is the total net input.

can have both positive and negative values
very many variations of neurons can have varying effects and said excitator and inhibitory values +/-

Absctacting Details:

Activation Functions
The net input can then be transformed into a rate code using an activation function
a commonly used function is the logistic sigmoid
f(net of j) = 1/(1+e^(-net of j))

posible to have negative weights therefore mirroring inhibitory effect

The chagne of post synaptic membrane potential is reflected in the net input

The amount of neurons changes how we view the item

if a layer has less than three units it is not impossible but passed that we must consider the hyperplane

how the layers of the nn work

input layer -> hidden layers -> hidden layers -> output layer

if no hidden layers are present, then it is a single layer 
input layer -> output layer

else it is a multilayer network

input layer -> hidden layers -> hidden layers -> output layer

problems where it difficult to come up with an algorithmic solutions

we are not trying to give the code to the net, learns through repeated exposure

it has been proven that given enough units and using only a single layer of hidden units, 
any function can be approximated to an arbitrary degree of precision

a single neuron will search for a single piece of the puzzle than pass its finds to the next layer 
this next layer will synthesize information from the first layer to search for a complex combination
this will repeat in complexity until the output layer with a hopefully correct answer

while it develops, it will also need to save that information

for simple problems it can be possible to hand wire them, but only works with simple problems

often to set up it is random 

we must know the derivative for this course

play around and do the second openlab to do scalars matrices and the like

will be important for the rest of the semester
----------------------------------------------------------------2/4/2020------------------------------------------------------------------------
Learning
Notice that most of the information encoded by a feed-forward network is in the connection(and bias) weights 
	So the question becomes how does one calculate the weights?
		handwired?
		trial and error
		rng byoy

The derivative
	a function is differentiable by its first partial derivative exists 
	everywhere in its domain for each of its arguments.
		a function is smooth if all partial derivatives exists

The Chain Rule
also works for partial derivatives
derivative of f(g(x)) is derivative of f(g(x)) times the derivative of g(x)
	r e m e m b e r  t h i s

Linear Review
A vector is similar to a 1-D array, but geometrically we think of a vector as a point in an n-dimensional
vector space, where n is the length of the vector.

concepts
	euclidean 
	cosine
	orthogonality

matrices
	a matrix is similar to a 2-D array
	multiplying a vector by a matrix produces another vector

the gradient and the hession

for a function of n variables the gradient is a vector

for a function of n variables the hessian is a matrix with each element

Pattern Recognition

the classic classification problem

the statistical pattern recognition problem
finding a function that properly sets up 

how does learning work and develop in the neuron,

neurons that fire together wire together.
	as they fire together the function dictates that the two values grw closer together
this is the fact of learning rules where when training neurons for a certain movement,
they recognize their importance in tandem and grow closer together in value

neurons fire, neurons self-update, test goes again